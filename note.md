- 一共有1521个文件，total data shape: (3040479, 31)
- 自己直接按照4：1随机划分文件作为验证集和测试集，随机种子为42，进行数据分析：
    - 逐个concat:251s, 全部读取后一道concat:15s, ThreadPoolExecutor并行读取：6s
    - 训练集和验证集都没有数据缺失
    - train: 2426786 val: 613693
    - 标签（没有去掉前100个数据和最后N个数据时）
        在训练集中：
        标签为0的样本个数： 367407
        标签为1的样本个数： 1694936
        标签为2的样本个数： 364443
        在测试集中：
        标签为0的样本个数： 93397
        标签为1的样本个数： 427412
        标签为2的样本个数： 92884
    - 现在针对不同的N从头开始训练不同的模型，因此并没有用到α
    - 经过手动标签标注后发现对于每个文件（每天、每个上下午）的最后N的标签应该是随机的，不是接着下一个半天的数据来标的，不要使用。
    - 应当使用的训练数据格式应当是：100个tick数据+label_N（每个文件不要最后N天的数据）。核心应当是如何压缩这100个tick数据的信息。如果是深度学习模型，就直接喂进去；对于其他模型
    - 每个文件的数据行数都为1999

- 策略
    1. 没有做任何数据处理，采用树模型。
    2. 

# TOOD
1. 检查涨跌停，处理盘口价格 