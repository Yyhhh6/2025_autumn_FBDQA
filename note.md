- 一共有1521个文件，total data shape: (3040479, 31)
- 自己直接按照4：1随机划分文件作为验证集和测试集，随机种子为42，进行数据分析：
    - 逐个concat:251s, 全部读取后一道concat:15s, ThreadPoolExecutor并行读取：6s
    - 训练集和验证集都没有数据缺失
    - train: 2426786 val: 613693
    - 标签（没有去掉前100个数据和最后N个数据时）
        在训练集中：
        标签为0的样本个数： 367407
        标签为1的样本个数： 1694936
        标签为2的样本个数： 364443
        在测试集中：
        标签为0的样本个数： 93397
        标签为1的样本个数： 427412
        标签为2的样本个数： 92884
    - 现在针对不同的N从头开始训练不同的模型，因此并没有用到α
    - 经过手动标签标注后发现对于每个文件（每天、每个上下午）的最后N的标签应该是随机的，不是接着下一个半天的数据来标的，不要使用。
    - 应当使用的训练数据格式应当是：100个tick数据+label_N（每个文件不要最后N天的数据）。核心应当是如何压缩这100个tick数据的信息。如果是深度学习模型，就直接喂进去；对于其他模型
    - 每个文件的数据行数都为1999

- 策略
    1. 没有做任何数据处理，采用树模型。
    2. 

# TOOD
1. 检查涨跌停，处理盘口价格 
2. 决策树的选择
    
    📌 总结对比
    | 特性     | XGBoost    | LightGBM    | CatBoost     |
    | ------ | ---------- | ----------- | ------------ |
    | 特点     | 稳定、全面、经典   | 更快、更轻、适合大数据 | 类别特征最强       |
    | 速度     | 快          | **最快**      | 中等           |
    | 调参难度   | 中等         | 较高（容易过拟合）   | **最简单**      |
    | 类别特征处理 | 弱          | 弱（要手动编码）    | **极强（自动处理）** |
    | 树构建方式  | Level-wise | Leaf-wise   | 对称树          |
    | 大规模训练  | 可以         | **非常好**     | 中等           |

    🧭 如何选择？
    如果你的数据是：

    - 类别特征多 → 直接用 CatBoost
    - 数据量大，追求速度 → LightGBM
    - 需要稳健的经典方案，调参空间大 → XGBoost


